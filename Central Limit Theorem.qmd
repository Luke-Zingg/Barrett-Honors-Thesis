---
title: "Central Limit Theorem"
format: html
editor: visual
---

Hello! As we wrap up our section on the Central Limit Theorem (CLT), we're gonna take a last look at how simulations in R adhere to what the CLT tells us about the distribution of sample means.

```{r,echo=FALSE}
#| output: false

# Load necessary libraries
library(ggplot2)
library(gridExtra)
library(dplyr)
```

## *With A Poisson Distribution:*

To demonstrate the CLT in action, we first need to define our population distribution. For this example, we’ll use a Poisson distribution with a rate parameter λ = 5. Remember, the Poisson distribution is discrete, but the CLT applies equally to both discrete and continuous distributions.

Next, we’ll specify the number of data points to sample from our population in each iteration of calculating the sample mean (x̄). In this case, we’ll draw 50 values from the Poisson distribution each time we compute x̄. With samples sizes of 50, we'll see that the standard deviation of the sampling distribution (i.e., the distribution of sample means) is smaller relative to that of our population, which is a key feature predicted by the CLT.

This result is intuitive. As we collect more data, our sample means become more precise estimates of the population mean. For instance, if we were to sample only two data points, their mean would likely differ substantially from the population mean due to random variation. In contrast, if we sample 50 data points, the sample mean is much more likely to be close to the true population mean.

Now, let’s let R handle the calculations. To illustrate the CLT, we will repeatedly sample 50 values from the population and compute x̄ for each sample. By leveraging R’s computational power, we can repeat this process 10,000 times. Finally, we will visualize both the population distribution and the distribution of the 10,000 sample means, showing how the CLT operates in practice.

In this instance, the red lines represent the means of both distributions. For the population distribution on the left, this is the population mean (µ). On the right, the red line represents the mean of our computed sample means. This is critical: the CLT says that the mean of all collected sample means ($\mu_{\bar{x}}$) is approximately equal to the population mean (µ).

```{r, echo=FALSE, warning=F}


# Define parameters
lambda <- 5    # Rate parameter of our Poisson distribution
sample_size <- 50   # Number of data points for each sample
num_samples <- 10000 # Number of means to collect

set.seed(150)

# Generate original dataset
original_dataset <- rpois(30000, lambda = lambda)

original_dataset <- ((original_dataset - mean(original_dataset)) * sqrt(lambda)/sd(original_dataset)) + lambda

# Theoretical mean and standard deviation for the population distribution
theoretical_mean <- mean(original_dataset)
theoretical_sd <- sd(original_dataset) / sqrt(sample_size)

# Convert original dataset to a data frame
original_dataframe <- data.frame(original_values = original_dataset)

# Generate means dataset using vectorized operations
generate_means_dataset <- function(original_data, sample_size, num_samples) {
  replicate(num_samples, mean(sample(original_data, sample_size, replace = TRUE)))
}

# Generate means dataset
means_dataset <- generate_means_dataset(original_dataset, sample_size, num_samples)
means_dataframe <- data.frame(means_values = means_dataset)

# Calculate means for both distributions
mean_original <- mean(original_dataset)
mean_means <- mean(means_dataframe$means_values)
sd_means <- sd(means_dataframe$means_values)

# Function to calculate number of bins using Sturges' rule
sturges_rule <- function(n) {
  ceiling(log2(n) + 1)
}

# Calculate maximum y-values for annotations
calculate_max_y <- function(dataframe, column) {
  max(ggplot_build(
    ggplot(dataframe, aes(x = !!sym(column))) +
      geom_histogram(bins = 2 * sturges_rule(nrow(dataframe)))
  )$data[[1]]$count)
}

# Get the max y values for annotations
max_y_original <- calculate_max_y(original_dataframe, "original_values")
max_y_means <- calculate_max_y(means_dataframe, "means_values")

# --- Create the first plot for the Population Distribution ---
original_plot <- ggplot(original_dataframe, aes(x = original_values)) +
  geom_histogram(bins = 2 * sturges_rule(nrow(original_dataframe)), 
                 fill = "#69b3a2", color = "black", alpha = 0.7) +
  annotate("segment", x = mean_original, xend = mean_original, y = 0, yend = max_y_original, 
           color = "red", linetype = "dashed", linewidth = 1) +
  annotate("text", x = mean_original + 3/4 * sd(original_dataframe$original_values), y = max_y_original * 0.9, 
           label = paste("µ =", round(mean_original, 3)), 
           color = "black", vjust = -0.5, hjust = -0.1, size = 5) +
  labs(title = "Population Distribution", x = "Values", y = "Frequency") +
  theme_bw(base_size = 15) +
  theme(plot.title = element_text(hjust = 0.5, size = 13))

# --- Create the second plot for the Sample Means Distribution ---
# --- Create the second plot for the Sample Means Distribution ---
means_plot <- ggplot(means_dataframe, aes(x = means_values)) +
  geom_histogram(bins = ceiling(2.8 * sturges_rule(nrow(means_dataframe))), 
                 fill = "#404080", color = "black", alpha = 0.7) +
  annotate("segment", x = mean_means, xend = mean_means, y = 0, yend = max_y_means, 
           color = "red", linetype = "dashed", linewidth = 1) +
  annotate("text", x = mean_means + 3/4 * sd(means_dataframe$means_values), y = max_y_means * 0.9, 
           label = bquote(mu[bar(x)] == .(round(mean_means, 3))),  # Use bquote here to evaluate mean_means
           color = "black", vjust = -0.5, hjust = -0.1, size = 5) +
  labs(title = "Distribution of Sample Means", x = "Values", y = "Frequency") +
  theme_bw(base_size = 15) +
  theme(plot.title = element_text(hjust = 0.5, size = 13))

# --- Arrange the two plots side by side ---
grid.arrange(original_plot, means_plot, ncol = 2)

```

Once again, we can see that even though the population wasn't normally distributed, the distribution of sample means is approximately normal. This is *exactly* what makes the Central Limit Theorem so powerful. The Central Limit Theorem says that the distribution of sample means will approximate a normal distribution with parameters, $X \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right)$. In this instance, this suggests the sampling distribution will follow $X \sim N\left(5, \frac{1}{\sqrt{10}}\right)$. We can see how our observed simulation follows this proposed distribution.

```{r,echo=FALSE}

# Find the theoretical normal distribution
means_values <- seq(lambda - 4 * sqrt(lambda)/sqrt(sample_size),
                    lambda + 4 * sqrt(lambda)/sqrt(sample_size),
                    length.out = 500)
means_perfect_normal <- dnorm(means_values, mean = mean_means, sd = sd_means)
means_perfect_normal_df <- data.frame(Values = means_values, Density = means_perfect_normal)

# Calculate bins using the Freedman-Diaconis rule
freedman_diaconis_bins <- function(data) {
  bin_width <- 2 * IQR(data) / (length(data)^(1/3))
  bins <- (max(data) - min(data)) / bin_width
  return(ceiling(bins))
}

fd_bins <- freedman_diaconis_bins(means_dataframe$means_values)

# Create the plot for Central Limit Theorem
clt_plot <- ggplot(means_dataframe, aes(x = means_values)) +
  geom_histogram(aes(y = after_stat(density)), bins = fd_bins, 
                 fill = "#6A5ACD", color = "black", alpha = 0.7) +
  geom_line(data = means_perfect_normal_df, aes(x = Values, y = Density), 
            color = "#FF4500", linewidth = 1.5) +
  annotate("text", x = mean_means + 2.2 * sd_means, y = max(means_perfect_normal_df$Density) * 0.6, 
           label = paste("X ~ N(",lambda, ",",round(sqrt(lambda), 3),")"), 
           color = "#FF4500", size = 5, angle = 0, vjust = -1.5, hjust = 0.5, fontface = "bold") +
  labs(title = "Central Limit Theorem Distribution", 
       subtitle = "Histogram of Sample Means with Theoretical Normal Distribution",
       x = "Value of Means", y = "Density") +
  theme_minimal(base_size = 15) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, face = "italic"),
        axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold"),
        panel.grid.major = element_line(color = "grey90"),
        panel.grid.minor = element_blank())

# Print the CLT plot
print(clt_plot)
```

We can see our observed sampling distribution follows quite closely to the normal distribution that is set by the CLT. This is because our sample size (n) is sufficiently large. As a rule of thumb, for n ≥ 30, the distribution of sample means follows closely to the CLT's normal distribution. In our case n = 50, giving us enough to adhere quite well to this distribution. We can compare the statistics from our distribution of sample means from our simulation to the normal distribution predicted by the Central Limit Theorem using the following table.

```{r,echo=FALSE}

# Calculate percentage differences
percentage_diff_mean <- ((mean_means - theoretical_mean) / theoretical_mean) * 100
percentage_diff_sd <- ((sd_means - theoretical_sd) / theoretical_sd) * 100

# Create a table with observed and theoretical values and percentage differences
table_data <- data.frame(
  Statistic = c("Mean:", "Standard Deviation:"),
  Observed = c(round(mean_means, 4), round(sd_means, 4)),
  Theoretical = c(round(theoretical_mean, 4), round(theoretical_sd, 4)),
  `Percentage Difference` = c(paste0(round(percentage_diff_mean, 2), "%"), paste0(round(percentage_diff_sd, 2), "%"))
)

# Customize the table theme for a vibrant look
custom_theme <- ttheme_default(
  core = list(
    bg_params = list(fill = c("#F7B076", "#748F67"), col = NA),
    fg_params = list(fontface = 3, fontsize = 15, hjust = 0.5)
  ),
  colhead = list(
    fg_params = list(col = "white", fontface = 2),
    bg_params = list(fill = "grey40", col = NA)
  )
)

# Create the table using tableGrob
table_grob <- tableGrob(table_data, rows = NULL, theme = custom_theme)

# Convert the tableGrob to a ggplot object
ggplot_table <- ggplot() +
  annotation_custom(table_grob) +
  theme_void()

# Print the ggplot table
print(ggplot_table)
```

We can see only a slight difference between the theoretical µ and $\frac{σ}{\sqrt{n}}$, and what our simulation actually yielded. Thus, the Central Limit Theorem is still true (phew!), and we've once again seen it in action.

## *With A Uniform Distribution:*

Finally, lets take a quick look at a different population distribution to get a feel for how the distribution of sample means really "tightens up" its range relative to the population, having a smaller standard deviation.

Take a look at the following histogram, showing the same process done previously for this uniformly distributed population.

```{r, warning=FALSE, echo=FALSE}
#Again we have our two variables. 
sample_size <- 40 #Sets the number of data points we pull from the population for each sample. This is the "n" in our normal distribution formula. 
num_samples <- 3000 #Sets the number of samples we calculate the mean of. 

# Additional demonstration with a uniform distribution
set.seed(124)

line_original_dataset <- runif(30000, min = 50, max = 250)
line_original_dataframe <- data.frame(line_original_dataset)

line_means_dataset <- generate_means_dataset(line_original_dataset, sample_size, num_samples)
line_means_dataframe <- data.frame(line_means_dataset)

line_original_df_plot <- ggplot(line_original_dataframe, aes(x = line_original_dataset, y = after_stat(density))) +
  geom_histogram(bins = 100, fill = "#1F618D", color = "black", alpha = 0.7) +
  geom_vline(aes(xintercept = mean(line_original_dataset)), linetype = "dashed", color = "black", linewidth = 1) +
  labs(title = "Population Distribution", x = "", y = "") +
  scale_x_continuous(limits = c(49, 251)) +
  theme_bw() +
  theme(axis.text.y = element_blank())

line_means_df_plot <- ggplot(line_means_dataframe, aes(x = line_means_dataset, y = after_stat(density))) +
  geom_histogram(bins = 150, fill = "#B03A2E", color = "black", alpha = 0.7) +
  geom_vline(aes(xintercept = mean(line_means_dataset)), linetype = "dashed", color = "black", linewidth = 1) +
  scale_x_continuous(limits = c(49, 251)) +
  labs(title = "Distribution of Sample Means", x = "x̄", y = "") +
  theme_bw() +
  theme(axis.text.y = element_blank())

grid.arrange(line_original_df_plot, line_means_df_plot, nrow = 2)
```

Notice how once again, the center of our sampling distribution (x̄), is essentially equal to the population mean (µ). However, the standard deviation of our distribution of sample means is significantly less than that of our population. This will *always* be the case. This is because, as previously mentioned, the sample means follow a normal distribution with a standard deviation equal to $\frac{σ}{\sqrt{n}}$. So, the standard deviation of the sampling distribution divides the population standard deviation (σ), by the square root of the sample size (n). This ensures that our distribution of sample means will be "tighter" than that of our population values.

Maybe by now these points feel a bit redundant. But if this is the case, just know that's exactly what we're hoping for! This puts you in a much better position for the rest of this course as compared to the many statstics students who don't fully grasp the CLT. The Central Limit Theorem is crucial to inferential statistics because of its reliability. The power to make a statement about a parameter regardless of the underlying distribution of our random variable is incredby rare. Statisticians capitalize on this, meaning the CLT is the backbone of confidence intervals and hypothesis testing. So, it's good to get a sense for how simulations fall in line with what it states. Frequently we'll just be citing it as a theorem that we need in order to conduct tests and make claims about population parameters. But it's important to see how it works in application, not just as some all-important theorem.
